{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Image Size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered image size: 495x619\n"
     ]
    }
   ],
   "source": [
    "def get_sample_image_size(dataset_dir):\n",
    "    \"\"\"\n",
    "    dataset_dir: path to something like \"Dataset/training\"\n",
    "                 which has subfolders (e.g., glioma_tumor, meningioma_tumor, etc.)\n",
    "    returns (width, height) of the first found image\n",
    "    \"\"\"\n",
    "    # List subfolders (the class folders)\n",
    "    class_folders = [f for f in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, f))]\n",
    "    \n",
    "    # We assume at least one class folder and at least one image\n",
    "    first_class_folder = os.path.join(dataset_dir, class_folders[0])\n",
    "    image_files = [f for f in os.listdir(first_class_folder) \n",
    "                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    first_image_path = os.path.join(first_class_folder, image_files[0])\n",
    "    \n",
    "    # Open the image and check size\n",
    "    with Image.open(first_image_path) as img:\n",
    "        width, height = img.size\n",
    "        print(f\"Discovered image size: {width}x{height}\")\n",
    "        return width, height\n",
    "\n",
    "# Example usage:\n",
    "dataset_dir = \"Dataset/testing\"\n",
    "detected_width, detected_height = get_sample_image_size(dataset_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5_512(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(LeNet5_512, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "        # 16×125×125 = 250,000 after the second pooling\n",
    "        self.fc1 = nn.Linear(16 * 125 * 125, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (3×512×512) => conv1 => (6×508×508) => pool => (6×254×254)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # => conv2 => (16×250×250) => pool => (16×125×125)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten => (batch_size, 250,000)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
      "Training samples: 2296\n",
      "Validation samples: 574\n",
      "Testing samples: 394\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example normalization (commonly used for color images, same as ImageNet)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "training_data_path = \"Dataset/training\"\n",
    "testing_data_path = \"Dataset/testing\"\n",
    "\n",
    "# Full training dataset (with train augmentation)\n",
    "full_train_dataset = datasets.ImageFolder(root=training_data_path, transform=train_transform)\n",
    "\n",
    "# Split into train/val (80/20)\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Apply simpler transform to validation set\n",
    "val_dataset.dataset.transform = val_test_transform\n",
    "\n",
    "# Test set\n",
    "test_dataset = datasets.ImageFolder(root=testing_data_path, transform=val_test_transform)\n",
    "\n",
    "batch_size = 8  # You may need to reduce batch_size if memory is an issue\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "class_names = full_train_dataset.classes\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Training samples:\", len(train_dataset))\n",
    "print(\"Validation samples:\", len(val_dataset))\n",
    "print(\"Testing samples:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        #################\n",
    "        # Training Phase\n",
    "        #################\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)   # shape: (batch_size, 4)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(preds == labels).item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        train_epoch_loss = running_loss / total_samples\n",
    "        train_epoch_acc = running_corrects / total_samples\n",
    "\n",
    "        ###################\n",
    "        # Validation Phase\n",
    "        ###################\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "        val_total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_images, val_labels in val_loader:\n",
    "                val_images = val_images.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "\n",
    "                val_outputs = model(val_images)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "                _, val_preds = torch.max(val_outputs, dim=1)\n",
    "                val_running_loss += val_loss.item() * val_images.size(0)\n",
    "                val_running_corrects += torch.sum(val_preds == val_labels).item()\n",
    "                val_total_samples += val_labels.size(0)\n",
    "\n",
    "        val_epoch_loss = val_running_loss / val_total_samples\n",
    "        val_epoch_acc = val_running_corrects / val_total_samples\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {train_epoch_loss:.4f}, Train Acc: {train_epoch_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize our LeNet5_512 model\n",
    "    model = LeNet5_512(num_classes=4)\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = train_model(model, train_loader, val_loader, num_epochs=5, lr=0.001)\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    trained_model.eval()\n",
    "    trained_model.to(device)\n",
    "\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = trained_model(images)  # shape: (batch_size, 4)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            test_correct += torch.sum(preds == labels).item()\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "    test_accuracy = test_correct / test_total\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Save model weights (optional)\n",
    "    torch.save(trained_model.state_dict(), \"brain_tumor_lenet_512.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
